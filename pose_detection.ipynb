{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2uE30coO-59",
        "outputId": "ecd5bc98-f0d9-46a1-b9b3-e126d8ef4394"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.11/dist-packages (0.10.21)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe numpy\n",
        "!pip install tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eO5y0UpSO-eV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeT3XkjuQE9D",
        "outputId": "6e9fb044-4231-4006-e85d-786a2e5ec290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Google Drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/mlp\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MU6oaACIO-eW"
      },
      "outputs": [],
      "source": [
        "# Helper function to calculate the angle between three points.\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"\n",
        "    Calculates the angle (in degrees) between the line segments ab and bc.\n",
        "\n",
        "    Args:\n",
        "        a, b, c (list or array): [x, y] coordinates.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated angle in degrees.\n",
        "    \"\"\"\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    c = np.array(c)\n",
        "\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    return np.degrees(angle)\n",
        "\n",
        "# Determine which arm to use based on the z-coordinates of the shoulders.\n",
        "def determine_arm(frame, pose_model):\n",
        "    \"\"\"\n",
        "    Determines which arm to use for analysis (left or right) by comparing\n",
        "    the z-coordinates of the left and right shoulders.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): A single video frame in BGR format.\n",
        "        pose_model: An instance of the Mediapipe Pose model.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the right arm should be used, False for the left.\n",
        "    \"\"\"\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose_model.process(image_rgb)\n",
        "    use_right_arm = True  # Default to right\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "        if landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER].z < \\\n",
        "           landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER].z:\n",
        "            use_right_arm = False\n",
        "\n",
        "    return use_right_arm\n",
        "\n",
        "# Process video frames and extract the elbow angles along with timestamps\n",
        "# and annotated frames.\n",
        "def process_video_frames(video_path):\n",
        "    \"\"\"\n",
        "    Processes video frames to extract the elbow angle using Mediapipe.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video.\n",
        "\n",
        "    Returns:\n",
        "        timestamps (np.array): Array of time stamps.\n",
        "        angles (np.array): Array of computed elbow angles.\n",
        "        annotated_frames (list): List of tuples (timestamp, frame) after drawing.\n",
        "        fps (int): Frames per second in the input video.\n",
        "        frame_width (int): Width of the video frames.\n",
        "        frame_height (int): Height of the video frames.\n",
        "    \"\"\"\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose_model = mp_pose.Pose()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Read the first frame to decide which arm to track.\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        pose_model.close()\n",
        "        raise ValueError(\"Unable to read video frame\")\n",
        "\n",
        "    use_right_arm = determine_arm(first_frame, pose_model)\n",
        "    shoulder_landmark = (mp_pose.PoseLandmark.RIGHT_SHOULDER if use_right_arm\n",
        "                         else mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
        "    elbow_landmark = (mp_pose.PoseLandmark.RIGHT_ELBOW if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_ELBOW)\n",
        "    wrist_landmark = (mp_pose.PoseLandmark.RIGHT_WRIST if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_WRIST)\n",
        "\n",
        "    # Reset to the beginning of the video.\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    angles = []\n",
        "    timestamps = []\n",
        "    annotated_frames = []\n",
        "    frame_idx = 0\n",
        "    print(\"Processing video frames...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        timestamp = frame_idx / fps\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose_model.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            shoulder = [landmarks[shoulder_landmark.value].x * frame_width,\n",
        "                        landmarks[shoulder_landmark.value].y * frame_height]\n",
        "            elbow = [landmarks[elbow_landmark.value].x * frame_width,\n",
        "                     landmarks[elbow_landmark.value].y * frame_height]\n",
        "            wrist = [landmarks[wrist_landmark.value].x * frame_width,\n",
        "                     landmarks[wrist_landmark.value].y * frame_height]\n",
        "\n",
        "            angle = calculate_angle(shoulder, elbow, wrist)\n",
        "            angles.append(angle)\n",
        "            timestamps.append(timestamp)\n",
        "\n",
        "            # Draw keypoints.\n",
        "            cv2.circle(frame, tuple(map(int, shoulder)), 8, (0, 255, 0), -1)\n",
        "            cv2.circle(frame, tuple(map(int, elbow)), 8, (255, 0, 0), -1)\n",
        "            cv2.circle(frame, tuple(map(int, wrist)), 8, (0, 0, 255), -1)\n",
        "\n",
        "            # Draw connecting lines.\n",
        "            cv2.line(frame, tuple(map(int, shoulder)),\n",
        "                     tuple(map(int, elbow)), (255, 255, 255), 2)\n",
        "            cv2.line(frame, tuple(map(int, elbow)),\n",
        "                     tuple(map(int, wrist)), (255, 255, 255), 2)\n",
        "\n",
        "            # Display the elbow angle.\n",
        "            cv2.putText(frame, f\"Angle: {int(angle)} deg\",\n",
        "                        (int(elbow[0]) + 20, int(elbow[1]) - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        annotated_frames.append((timestamp, frame))\n",
        "        print(f\"\\rProgress: {(frame_idx/total_frames)*100:.1f}%\", end=\"\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    cap.release()\n",
        "    pose_model.close()\n",
        "\n",
        "    return np.array(timestamps), np.array(angles), annotated_frames, fps, frame_width, frame_height\n",
        "\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def detect_reps(angles, timestamps, sigma=10, min_distance=50, min_valley_diff=60):\n",
        "    \"\"\"\n",
        "    Detects bicep curl repetitions by identifying peaks (high points) and valleys\n",
        "    (low points) dynamically.\n",
        "\n",
        "    Args:\n",
        "        angles (list or np.array): The elbow angles over time.\n",
        "        timestamps (list or np.array): The corresponding timestamps.\n",
        "        sigma (float): Smoothing factor for the angles.\n",
        "        min_distance (int): Minimum distance between peaks/valleys (in samples).\n",
        "        min_valley_diff (int): Minimum angle difference required between peak and valley.\n",
        "\n",
        "    Returns:\n",
        "        rep_segments (list): List of tuples (start_time, end_time) for each rep.\n",
        "        angles_smoothed (np.array): The smoothed angle data.\n",
        "    \"\"\"\n",
        "    # Smooth the angles to reduce noise\n",
        "    angles_smoothed = scipy.ndimage.gaussian_filter1d(angles, sigma=sigma)\n",
        "\n",
        "    # Find peaks (local maxima) and valleys (local minima)\n",
        "    peaks, _ = find_peaks(angles_smoothed, distance=min_distance, height=90)  # Only peaks above 90 degrees\n",
        "    valleys, _ = find_peaks(-angles_smoothed, distance=min_distance, height=(-90))  # Inverted signal for valleys, must be below 90\n",
        "\n",
        "    # Validate peaks and valleys\n",
        "    valid_peaks = []\n",
        "    valid_valleys = []\n",
        "    for i in range(len(peaks)-1):\n",
        "        # Find valleys between consecutive peaks\n",
        "        valley_candidates = [v for v in valleys if peaks[i] < v < peaks[i+1]]\n",
        "\n",
        "        if valley_candidates:\n",
        "            # If multiple valleys exist, keep only the one with highest angle (shallowest valley)\n",
        "            valley = max(valley_candidates, key=lambda v: angles_smoothed[v])\n",
        "\n",
        "            # Check if valley is deep enough compared to both peaks\n",
        "            peak1_to_valley = angles_smoothed[peaks[i]] - angles_smoothed[valley]\n",
        "            valley_to_peak2 = angles_smoothed[peaks[i+1]] - angles_smoothed[valley]\n",
        "\n",
        "            if peak1_to_valley >= min_valley_diff and valley_to_peak2 >= min_valley_diff:\n",
        "                valid_peaks.append(peaks[i])\n",
        "                valid_valleys.append(valley)\n",
        "                if i == len(peaks)-2:  # Add the last peak if its valley was valid\n",
        "                    valid_peaks.append(peaks[i+1])\n",
        "\n",
        "    # Create rep segments from valid peaks\n",
        "    rep_segments = []\n",
        "    if valid_peaks:\n",
        "        # First segment starts from beginning to first valid peak\n",
        "        rep_segments.append((timestamps[0], timestamps[valid_peaks[0]]))\n",
        "\n",
        "        # Middle segments\n",
        "        for i in range(len(valid_peaks)-1):\n",
        "            rep_segments.append((timestamps[valid_peaks[i]], timestamps[valid_peaks[i+1]]))\n",
        "\n",
        "        # Last segment from last valid peak to end\n",
        "        rep_segments.append((timestamps[valid_peaks[-1]], timestamps[-1]))\n",
        "\n",
        "    return rep_segments\n",
        "\n",
        "# Plot the elbow angle versus time and mark the detected repetitions.\n",
        "def plot_elbow_angle(timestamps, angles_smoothed, rep_segments, file_name):\n",
        "    \"\"\"\n",
        "    Plots the smoothed elbow angle over time and highlights the periods\n",
        "    considered as repetitions.\n",
        "\n",
        "    Args:\n",
        "        timestamps (np.array): Array of timestamps.\n",
        "        angles_smoothed (np.array): Smoothed elbow angle values.\n",
        "        rep_segments (list): List of rep segments as (start_time, end_time).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(timestamps, angles_smoothed, 'b-', label='Elbow Angle')\n",
        "\n",
        "    colors=['green', 'red']\n",
        "\n",
        "    # Highlight the periods considered as repetitions\n",
        "    for i, (start, end) in enumerate(rep_segments):\n",
        "        plt.axvspan(start, end, color=colors[i % len(colors)], alpha=0.3, label='Rep Period')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Elbow Angle (degrees)')\n",
        "    plt.title(f'Bicep Curl Analysis - {file_name}')\n",
        "    plt.legend(['Elbow Angle', 'Rep Period'], loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.gca().invert_yaxis()  # Reverse the y-axis\n",
        "    plt.show()\n",
        "\n",
        "# Overlay repetition markers on the annotated video frames.\n",
        "def overlay_rep_markers(annotated_frames, rep_segments, fps):\n",
        "    \"\"\"\n",
        "    Overlays rep start and end markers on each annotated frame.\n",
        "\n",
        "    Args:\n",
        "        annotated_frames (list): List of tuples (timestamp, frame).\n",
        "        rep_segments (list): List of rep segments as (start_time, end_time).\n",
        "        fps (int): Frames per second.\n",
        "\n",
        "    Returns:\n",
        "        List of frames with overlay annotations.\n",
        "    \"\"\"\n",
        "    updated_frames = []\n",
        "    for timestamp, frame in annotated_frames:\n",
        "        # Find current rep\n",
        "        current_rep = 0\n",
        "        for i, (start, end) in enumerate(rep_segments):\n",
        "            if start <= timestamp <= end:\n",
        "                current_rep = i + 1\n",
        "            if abs(timestamp - start) < 1 / fps:\n",
        "                cv2.circle(frame, (50, 50), 15, (0, 255, 0), -1)\n",
        "                cv2.putText(frame, f\"Start Rep {i+1}\", (70, 55),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            if abs(timestamp - end) < 1 / fps:\n",
        "                cv2.circle(frame, (50, 100), 15, (0, 0, 255), -1)\n",
        "                cv2.putText(frame, f\"End Rep {i+1}\", (70, 105),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Draw stats overlay\n",
        "        y_offset = 30\n",
        "        cv2.putText(frame, f\"Time: {timestamp:.1f}s\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        y_offset += 25\n",
        "        cv2.putText(frame, f\"Current Rep: {current_rep}\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        y_offset += 25\n",
        "        cv2.putText(frame, f\"Total Reps: {len(rep_segments)}\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        if current_rep > 0 and current_rep <= len(rep_segments):\n",
        "            start, end = rep_segments[current_rep-1]\n",
        "            y_offset += 25\n",
        "            cv2.putText(frame, f\"Rep Duration: {end-start:.1f}s\", (10, y_offset),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        updated_frames.append(frame)\n",
        "    return updated_frames\n",
        "\n",
        "# Write the annotated frames to a video file.\n",
        "def write_video(frames, output_path, fps, frame_width, frame_height):\n",
        "    \"\"\"\n",
        "    Writes a list of frames to a video file.\n",
        "\n",
        "    Args:\n",
        "        frames (list): List of frames (images).\n",
        "        output_path (str): Path to save the output video.\n",
        "        fps (int): Frames per second.\n",
        "        frame_width (int): Width of video frames.\n",
        "        frame_height (int): Height of video frames.\n",
        "    \"\"\"\n",
        "    out = cv2.VideoWriter(\n",
        "        output_path,\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps,\n",
        "        (frame_width, frame_height)\n",
        "    )\n",
        "    for frame in frames:\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "    print(f\"âœ… Processed video saved as `{output_path}`\")\n",
        "\n",
        "# Main routine that ties everything together.\n",
        "def run_bicep_curl_analysis(video_path, output_path=\"bicep_curl_debug.mp4\"):\n",
        "    \"\"\"\n",
        "    Orchestrates the bicep curl analysis pipeline:\n",
        "      1. Process video frames to extract angles.\n",
        "      2. Detect reps via signal processing.\n",
        "      3. Plot the elbow angle versus time.\n",
        "      4. Overlay rep markers on the video.\n",
        "      5. Write the annotated video to disk.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video.\n",
        "        output_path (str): Path to save the annotated output video.\n",
        "\n",
        "    Returns:\n",
        "        list: List of rep segments (each as (start_time, end_time)).\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "        process_video_frames(video_path)\n",
        "    rep_segments = detect_reps(angles, timestamps)\n",
        "\n",
        "    print(\"\\nDetected reps:\")\n",
        "    for i, (start, end) in enumerate(rep_segments):\n",
        "        print(f\"Rep {i+1}: Start = {start:.2f}s, End = {end:.2f}s, \"\n",
        "              f\"Duration = {end - start:.2f}s\")\n",
        "\n",
        "    plot_elbow_angle(timestamps, angles, rep_segments, video_path)\n",
        "    frames_with_overlay = overlay_rep_markers(annotated_frames, rep_segments, fps)\n",
        "    write_video(frames_with_overlay, output_path, fps, frame_width, frame_height)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTotal processing time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return rep_segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tgyMV7GBO-eY"
      },
      "outputs": [],
      "source": [
        "# # timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "# #         process_video_frames(\"./data/12r (1).mp4\")\n",
        "\n",
        "# files = os.listdir(\"./data\")\n",
        "# print(files)\n",
        "\n",
        "# # for each file in ./data folder, run the analyze_bicep_curls function\n",
        "# analysis_results = []\n",
        "# for file in files:\n",
        "#     if file.endswith(\".mp4\"):\n",
        "#         print(f\"Analyzing {file}\")\n",
        "#         timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "#             process_video_frames(\"./data/\" + file)\n",
        "#         analysis_results.append((file, timestamps, angles, fps, frame_width, frame_height))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BPT3-70OO-ea"
      },
      "outputs": [],
      "source": [
        "# for file, timestamps, angles, fps, frame_width, frame_height in analysis_results:\n",
        "#     rep_segments = detect_reps(angles, timestamps)\n",
        "#     plot_elbow_angle(timestamps, angles, rep_segments, \"./data/\" + file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CrYCTV-UO-ea"
      },
      "outputs": [],
      "source": [
        "# files = os.listdir(\"./data\")\n",
        "# print(files)\n",
        "\n",
        "# # for each file in ./data folder, run the analyze_bicep_curls function\n",
        "# for file in files:\n",
        "#     if file.endswith(\".mp4\") and \"t\" in file:\n",
        "#         print(f\"Analyzing {file}\")\n",
        "#         run_bicep_curl_analysis(f\"./data/{file}\", f\"./out/{file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tea_Urmlt5H4"
      },
      "outputs": [],
      "source": [
        "def extract_rep_joint_data(video_path, normalize=True):\n",
        "    \"\"\"\n",
        "    Extracts joint coordinate data for each repetition from a video.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video\n",
        "        normalize (bool): Whether to normalize coordinates relative to shoulder position\n",
        "\n",
        "    Returns:\n",
        "        list: List of repetitions, where each rep is a numpy array of shape\n",
        "             (timesteps, n_features). Features are [elbow_x, elbow_y, wrist_x, wrist_y]\n",
        "             or their normalized versions relative to shoulder position.\n",
        "    \"\"\"\n",
        "    # Process video to get landmarks\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose_model = mp_pose.Pose()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Determine which arm to use\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        pose_model.close()\n",
        "        raise ValueError(\"Unable to read video frame\")\n",
        "\n",
        "    use_right_arm = determine_arm(first_frame, pose_model)\n",
        "    shoulder_landmark = (mp_pose.PoseLandmark.RIGHT_SHOULDER if use_right_arm\n",
        "                        else mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
        "    elbow_landmark = (mp_pose.PoseLandmark.RIGHT_ELBOW if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_ELBOW)\n",
        "    wrist_landmark = (mp_pose.PoseLandmark.RIGHT_WRIST if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_WRIST)\n",
        "\n",
        "    # Reset video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    # Extract coordinates and angles\n",
        "    coords = []\n",
        "    angles = []\n",
        "    timestamps = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        timestamp = frame_idx / fps\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose_model.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            shoulder = np.array([\n",
        "                landmarks[shoulder_landmark.value].x * frame_width,\n",
        "                landmarks[shoulder_landmark.value].y * frame_height\n",
        "            ])\n",
        "            elbow = np.array([\n",
        "                landmarks[elbow_landmark.value].x * frame_width,\n",
        "                landmarks[elbow_landmark.value].y * frame_height\n",
        "            ])\n",
        "            wrist = np.array([\n",
        "                landmarks[wrist_landmark.value].x * frame_width,\n",
        "                landmarks[wrist_landmark.value].y * frame_height\n",
        "            ])\n",
        "\n",
        "            if normalize:\n",
        "                # Normalize coordinates relative to shoulder position\n",
        "                elbow = elbow - shoulder\n",
        "                wrist = wrist - shoulder\n",
        "\n",
        "            # Store coordinates as [elbow_x, elbow_y, wrist_x, wrist_y]\n",
        "            coords.append(np.concatenate([elbow, wrist]))\n",
        "\n",
        "            angle = calculate_angle(shoulder, elbow + shoulder, wrist + shoulder if normalize else wrist)\n",
        "            angles.append(angle)\n",
        "            timestamps.append(timestamp)\n",
        "\n",
        "    cap.release()\n",
        "    pose_model.close()\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    coords = np.array(coords)\n",
        "    angles = np.array(angles)\n",
        "    timestamps = np.array(timestamps)\n",
        "\n",
        "    # Detect repetitions\n",
        "    rep_segments = detect_reps(angles, timestamps)\n",
        "\n",
        "    # Split coordinates into repetitions\n",
        "    reps_data = []\n",
        "    for start_time, end_time in rep_segments:\n",
        "        start_idx = np.argmin(np.abs(timestamps - start_time))\n",
        "        end_idx = np.argmin(np.abs(timestamps - end_time))\n",
        "        rep_coords = coords[start_idx:end_idx+1]\n",
        "        reps_data.append(rep_coords)\n",
        "\n",
        "    return reps_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpNxidLt_5m",
        "outputId": "0ad5ad9e-56f4-4379-d39e-029d60ce1260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9FiUH5c0O-ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_all_videos(data_dir):\n",
        "    \"\"\"\n",
        "    Process all videos in a directory and return their rep data.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to directory containing videos\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping filenames to lists of rep data arrays\n",
        "    \"\"\"\n",
        "    all_sets_data = {}\n",
        "\n",
        "    for file in os.listdir(data_dir)[1:]:\n",
        "        if file.endswith('.mp4'):\n",
        "            print(f\"Processing {file}...\")\n",
        "            video_path = os.path.join(data_dir, file)\n",
        "            sets_data = extract_rep_joint_data(video_path)\n",
        "            all_sets_data[file] = sets_data\n",
        "            print(f\"Found {len(sets_data)} reps in {file}\")\n",
        "    return all_sets_data\n",
        "\n",
        "    # Get the first video file\n",
        "    # files = [f for f in os.listdir(data_dir) if f.endswith('.mp4')]\n",
        "    # if files:\n",
        "    #     first_file = files[0]\n",
        "    #     print(f\"Processing {first_file}...\")\n",
        "    #     video_path = os.path.join(data_dir, first_file)\n",
        "    #     reps_data = extract_rep_joint_data(video_path)\n",
        "\n",
        "    #     # Print info about the extracted data\n",
        "    #     print(f\"\\nFound {len(reps_data)} reps in {first_file}\")\n",
        "    #     for i, rep in enumerate(reps_data):\n",
        "    #         print(f\"Rep {i+1} shape: {rep.shape}\")\n",
        "    # else:\n",
        "    #     print(\"No video files found in ./data directory\")\n",
        "\n",
        "    # return reps_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCL0VTYzuR-b",
        "outputId": "f40625da-6d4a-4a06-a94b-ba1432901ba6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing t5.mp4...\n",
            "Found 19 reps in t5.mp4\n",
            "Processing t1.mp4...\n",
            "Found 11 reps in t1.mp4\n",
            "Processing t2.mp4...\n",
            "Found 20 reps in t2.mp4\n",
            "Processing 5_l.mp4...\n",
            "Found 11 reps in 5_l.mp4\n",
            "Processing 14r (1).mp4...\n",
            "Found 9 reps in 14r (1).mp4\n",
            "Processing 11r (1).mp4...\n",
            "Found 5 reps in 11r (1).mp4\n",
            "Processing 12r (1).mp4...\n",
            "Found 21 reps in 12r (1).mp4\n",
            "Processing 5_r.mp4...\n",
            "Found 16 reps in 5_r.mp4\n",
            "Processing 10r (1).mp4...\n",
            "Found 3 reps in 10r (1).mp4\n",
            "Processing 8_r (1).mp4...\n",
            "Found 5 reps in 8_r (1).mp4\n",
            "Processing t4.mp4...\n",
            "Found 17 reps in t4.mp4\n",
            "Processing 6_l.mp4...\n",
            "Found 11 reps in 6_l.mp4\n",
            "Processing t6.mp4...\n",
            "Found 18 reps in t6.mp4\n",
            "Processing 9r (1).mp4...\n",
            "Found 13 reps in 9r (1).mp4\n",
            "Processing 7_r (1).mp4...\n",
            "Found 5 reps in 7_r (1).mp4\n",
            "Processing t3.mp4...\n",
            "Found 12 reps in t3.mp4\n",
            "Processing 13r (1).mp4...\n",
            "Found 10 reps in 13r (1).mp4\n"
          ]
        }
      ],
      "source": [
        "all_sets_data = process_all_videos(\"./data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Po5yCfnz9v_r"
      },
      "outputs": [],
      "source": [
        "def save_video_reps(video_name, reps_data, save_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Save rep data for a single video to a .npz file.\n",
        "\n",
        "    Args:\n",
        "        video_name (str): Name of the video file\n",
        "        reps_data (list): List of numpy arrays containing rep data\n",
        "        save_dir (str): Directory to save the .npz files\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, video_name.replace('.mp4', '.npz'))\n",
        "    save_dict = {f'rep_{i}': rep for i, rep in enumerate(reps_data)}\n",
        "    np.savez_compressed(save_path, **save_dict)\n",
        "\n",
        "def process_videos_directory(data_dir, save_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Process all videos in directory, skipping those that already have .npz files.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing videos\n",
        "        save_dir (str): Directory to save processed data\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of processed videos and their rep data\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    all_sets_data = {}\n",
        "\n",
        "    for file in os.listdir(data_dir):\n",
        "        if file.endswith('.mp4'):\n",
        "            npz_path = os.path.join(save_dir, file.replace('.mp4', '.npz'))\n",
        "\n",
        "            if os.path.exists(npz_path):\n",
        "                print(f\"Skipping {file} - already processed\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing {file}...\")\n",
        "            video_path = os.path.join(data_dir, file)\n",
        "            set_data = extract_rep_joint_data(video_path)\n",
        "            all_sets_data[file] = set_data\n",
        "\n",
        "            # Save the processed data\n",
        "            save_video_reps(file, set_data, save_dir)\n",
        "            print(f\"Found and saved {len(set_data)} reps for {file}\")\n",
        "\n",
        "    return all_sets_data\n",
        "\n",
        "def save_existing_reps_data(all_reps_data, save_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Save any rep data currently in memory that hasn't been saved yet.\n",
        "\n",
        "    Args:\n",
        "        all_reps_data (dict): Dictionary mapping video names to rep data\n",
        "        save_dir (str): Directory to save processed data\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for video_name, set_data in all_reps_data.items():\n",
        "        npz_path = os.path.join(save_dir, video_name.replace('.mp4', '.npz'))\n",
        "        if not os.path.exists(npz_path):\n",
        "            print(f\"Saving data for {video_name}...\")\n",
        "            save_video_reps(video_name, set_data, save_dir)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_all_processed_data(processed_dir='processed_data'):\n",
        "    \"\"\"\n",
        "    Load all processed rep data from .npz files in directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping video names to a list of rep arrays.\n",
        "              Each rep array should ideally have shape (timesteps, 4).\n",
        "    \"\"\"\n",
        "    all_sets_data = {}\n",
        "\n",
        "    for file in os.listdir(processed_dir):\n",
        "        if file.endswith('.npz'):\n",
        "            video_name = file.replace('.npz', '.mp4')\n",
        "            npz_path = os.path.join(processed_dir, file)\n",
        "\n",
        "            loaded = np.load(npz_path)\n",
        "\n",
        "            # Let's store arrays in a list\n",
        "            reps = []\n",
        "            for key in sorted(loaded.files):\n",
        "                arr = loaded[key]\n",
        "\n",
        "                # Debug info: check array type and shape\n",
        "                print(f\"{file} -> key={key}, arr.shape={arr.shape}, arr.dtype={arr.dtype}\")\n",
        "\n",
        "                reps.append(arr)\n",
        "\n",
        "            all_sets_data[video_name] = reps\n",
        "            print(f\"Loaded {len(reps)} reps from {video_name}\")\n",
        "\n",
        "    return all_sets_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "VA2iH1H52OPH"
      },
      "outputs": [],
      "source": [
        "def prepare_rep_for_lstm(rep_data, fps=30):\n",
        "    \"\"\"\n",
        "    Prepares a single repetition's joint data for LSTM input by:\n",
        "    1. Computing ROM (Range of Motion)\n",
        "    2. Computing velocities\n",
        "    3. Adding rep duration\n",
        "\n",
        "    Args:\n",
        "        rep_data (np.ndarray): Array of shape (timesteps, 4) containing\n",
        "                              [elbow_x, elbow_y, wrist_x, wrist_y]\n",
        "        fps (int): Frames per second of the video\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Processed features of shape (timesteps, n_features)\n",
        "                   Features: [norm_elbow_x, norm_elbow_y, norm_wrist_x, norm_wrist_y,\n",
        "                            elbow_velocity, wrist_velocity, rom_percentage]\n",
        "    \"\"\"\n",
        "\n",
        "    normalized_coords = rep_data\n",
        "\n",
        "\n",
        "    # 2. Calculate ROM (Range of Motion)\n",
        "    # Use Euclidean distances from elbow to wrist\n",
        "    wrist_to_elbow = np.sqrt(\n",
        "        (rep_data[:, 2] - rep_data[:, 0])**2 +\n",
        "        (rep_data[:, 3] - rep_data[:, 1])**2\n",
        "    )\n",
        "    rom = wrist_to_elbow - np.min(wrist_to_elbow)\n",
        "    rom_percentage = rom / np.max(rom)\n",
        "\n",
        "    # 3. Calculate velocities (change in position per frame)\n",
        "    dt = 1/fps\n",
        "\n",
        "    # Elbow velocity (magnitude)\n",
        "    elbow_velocity = np.sqrt(\n",
        "        np.gradient(normalized_coords[:, 0])**2 +\n",
        "        np.gradient(normalized_coords[:, 1])**2\n",
        "    ) / dt\n",
        "\n",
        "    # Wrist velocity (magnitude)\n",
        "    wrist_velocity = np.sqrt(\n",
        "        np.gradient(normalized_coords[:, 2])**2 +\n",
        "        np.gradient(normalized_coords[:, 3])**2\n",
        "    ) / dt\n",
        "\n",
        "\n",
        "\n",
        "    # Combine all features\n",
        "    features = np.column_stack([\n",
        "        normalized_coords,  # [norm_elbow_x, norm_elbow_y, norm_wrist_x, norm_wrist_y]\n",
        "        elbow_velocity,     # elbow velocity magnitude\n",
        "        wrist_velocity,     # wrist velocity magnitude\n",
        "        rom_percentage,     # range of motion percentage\n",
        "    ])\n",
        "\n",
        "    return features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JhDz5aI53aB",
        "outputId": "730e2e16-4858-4c81-9797-d827b5fd03ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature statistics:\n",
            "--------------------------------------------------\n",
            "Normalized Elbow X: min= 0.000, max= 1.000, mean= 0.490\n",
            "Normalized Elbow Y: min= 0.000, max= 1.000, mean= 0.593\n",
            "Normalized Wrist X: min= 0.000, max= 1.000, mean= 0.593\n",
            "Normalized Wrist Y: min= 0.000, max= 1.000, mean= 0.506\n",
            "Elbow Velocity : min= 0.140, max= 5.380, mean= 1.696\n",
            "Wrist Velocity : min= 0.161, max= 2.457, mean= 1.302\n",
            "ROM Percentage : min= 0.000, max= 1.000, mean= 0.508\n",
            "Rep Duration   : min= 2.533, max= 2.533, mean= 2.533\n"
          ]
        }
      ],
      "source": [
        "def inspect_rep(processed_rep):\n",
        "    \"\"\"\n",
        "    Prints basic statistics for a processed repetition.\n",
        "\n",
        "    Args:\n",
        "        processed_rep (np.ndarray): Processed features of shape (timesteps, 8)\n",
        "    \"\"\"\n",
        "    feature_names = [\n",
        "        \"Normalized Elbow X\",\n",
        "        \"Normalized Elbow Y\",\n",
        "        \"Normalized Wrist X\",\n",
        "        \"Normalized Wrist Y\",\n",
        "        \"Elbow Velocity\",\n",
        "        \"Wrist Velocity\",\n",
        "        \"ROM Percentage\",\n",
        "        \"Rep Duration\"\n",
        "    ]\n",
        "\n",
        "    print(\"Feature statistics:\")\n",
        "    print(\"-\" * 50)\n",
        "    for i, name in enumerate(feature_names):\n",
        "        values = processed_rep[:, i]\n",
        "        print(f\"{name:15s}: min={values.min():6.3f}, max={values.max():6.3f}, mean={values.mean():6.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bkppclOQ-2HW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def prepare_sets_for_lstm_separate(all_sets_data, fps=30):\n",
        "    \"\"\"\n",
        "    Processes all sets of repetitions for LSTM training without flattening them.\n",
        "    Each set remains separate so the model can predict which rep is the failing rep.\n",
        "\n",
        "    Args:\n",
        "        all_sets_data (list):\n",
        "            A list of sets, where each set_data is itself a list of reps.\n",
        "            Each rep is a numpy array of shape (timesteps, 4)\n",
        "            for [elbow_x, elbow_y, wrist_x, wrist_y].\n",
        "        fps (int): Frames per second for velocity calculations.\n",
        "\n",
        "    Returns:\n",
        "        X_padded: np.ndarray of shape (num_sets, max_reps, max_timesteps, 7).\n",
        "        Y:        np.ndarray of shape (num_sets, max_reps),\n",
        "                  where Y[i, j] = 1 if rep j is the last successful rep in set i,\n",
        "                                  0 for normal reps, and\n",
        "                                  -1 if there is no rep (padded).\n",
        "        rep_counts: list of actual number of reps in each set (useful for masking).\n",
        "    \"\"\"\n",
        "    # 1) Process each rep\n",
        "    all_processed_sets = []\n",
        "    for set_data in all_sets_data:\n",
        "        processed_set = [prepare_rep_for_lstm(rep, fps=fps) for rep in set_data]\n",
        "        all_processed_sets.append(processed_set)\n",
        "\n",
        "    # 2) Find the max number of reps and max number of timesteps\n",
        "    max_reps = max(len(s) for s in all_processed_sets)\n",
        "    max_timesteps = max(\n",
        "        rep.shape[0]\n",
        "        for set_reps in all_processed_sets\n",
        "        for rep in set_reps\n",
        "    )\n",
        "\n",
        "    num_sets = len(all_processed_sets)\n",
        "\n",
        "    # 3) Initialize arrays\n",
        "    #    X_padded: (num_sets, max_reps, max_timesteps, 7)\n",
        "    #    Y:        (num_sets, max_reps)\n",
        "    #              (will contain 0/1 for real reps, and -1 for padded reps)\n",
        "    X_padded = np.zeros((num_sets, max_reps, max_timesteps, 7), dtype=np.float32)\n",
        "    Y = -1 * np.ones((num_sets, max_reps), dtype=np.int32)  # -1 for no rep\n",
        "\n",
        "    rep_counts = []  # Keep track of how many reps each set has\n",
        "\n",
        "    # 4) Fill in data and labels\n",
        "    for i, set_reps in enumerate(all_processed_sets):\n",
        "        rep_counts.append(len(set_reps))\n",
        "        for j, rep in enumerate(set_reps):\n",
        "            # Rep shape: (timesteps, 7)\n",
        "            timesteps = rep.shape[0]\n",
        "\n",
        "            # Copy repâ€™s data into X_padded\n",
        "            X_padded[i, j, :timesteps, :] = rep\n",
        "\n",
        "            # Last rep gets label=1, otherwise 0\n",
        "            if j == len(set_reps) - 1:\n",
        "                Y[i, j] = 1\n",
        "            else:\n",
        "                Y[i, j] = 0\n",
        "\n",
        "    return X_padded, Y, rep_counts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cSXMlfddPaHP"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def build_two_level_lstm_model(\n",
        "    max_reps,\n",
        "    max_timesteps,\n",
        "    feature_dim=7,\n",
        "    rep_embedding_dim=32,\n",
        "    hidden_units=32\n",
        "):\n",
        "    \"\"\"\n",
        "    Builds a two-level LSTM model:\n",
        "    1) Inner LSTM that processes each rep across timesteps -> rep embedding\n",
        "    2) Outer LSTM that processes the sequence of rep embeddings -> outputs classification for each rep.\n",
        "    \"\"\"\n",
        "\n",
        "    # Input shape: (batch_size, max_reps, max_timesteps, feature_dim)\n",
        "    input_tensor = tf.keras.Input(\n",
        "        shape=(max_reps, max_timesteps, feature_dim),\n",
        "        name=\"input\"\n",
        "    )\n",
        "\n",
        "    # (1) Inner LSTM: We use TimeDistributed to apply the same LSTM to each rep:\n",
        "    #     - For each rep, shape is (max_timesteps, feature_dim).\n",
        "    #     - return_sequences=False => we get a single embedding per rep.\n",
        "    inner_lstm = layers.TimeDistributed(\n",
        "        layers.LSTM(rep_embedding_dim, return_sequences=False),\n",
        "        name=\"inner_lstm\"\n",
        "    )(input_tensor)\n",
        "    # inner_lstm shape: (batch_size, max_reps, rep_embedding_dim)\n",
        "\n",
        "    # (2) Outer LSTM over the sequence of rep embeddings:\n",
        "    #     - return_sequences=True => a rep-level output for each rep (so we can classify each rep).\n",
        "    outer_lstm = layers.LSTM(hidden_units, return_sequences=True, name=\"outer_lstm\")(inner_lstm)\n",
        "    # outer_lstm shape: (batch_size, max_reps, hidden_units)\n",
        "\n",
        "    # (3) Output classifier: TimeDistributed Dense -> shape: (batch_size, max_reps, 1)\n",
        "    outputs = layers.TimeDistributed(\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "        name=\"classifier\"\n",
        "    )(outer_lstm)\n",
        "\n",
        "    model = models.Model(inputs=input_tensor, outputs=outputs)\n",
        "    model.compile(\n",
        "        loss='binary_crossentropy',\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model.summary()\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5adT03dTVG-U"
      },
      "outputs": [],
      "source": [
        "all_sets = load_all_processed_data(\"./processed_npzs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UMn1bVnQDIB",
        "outputId": "d5875c98-2d31-41c9-f100-8d57e1bff6c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_padded.shape: (17, 21, 444, 7)\n"
          ]
        }
      ],
      "source": [
        "# 1) Prepare your data:\n",
        "all_sets_list = list(all_sets.values())\n",
        "\n",
        "X_padded, Y, rep_counts = prepare_sets_for_lstm_separate(all_sets_list, fps=30)\n",
        "\n",
        "# X_padded has shape: (num_sets, max_reps, max_timesteps, 7).\n",
        "print(\"X_padded.shape:\", X_padded.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "id": "ZE92iNEvRtb-",
        "outputId": "c0790fec-7e25-4d3b-fb19-14b97bdbe240"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">444</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)          â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ inner_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ outer_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ classifier (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)               â”‚              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input (\u001b[38;5;33mInputLayer\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m444\u001b[0m, \u001b[38;5;34m7\u001b[0m)          â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ inner_lstm (\u001b[38;5;33mTimeDistributed\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)              â”‚           \u001b[38;5;34m5,120\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ outer_lstm (\u001b[38;5;33mLSTM\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m32\u001b[0m)              â”‚           \u001b[38;5;34m8,320\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ classifier (\u001b[38;5;33mTimeDistributed\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m1\u001b[0m)               â”‚              \u001b[38;5;34m33\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,473</span> (52.63 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,473\u001b[0m (52.63 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,473</span> (52.63 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,473\u001b[0m (52.63 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.6802\n",
            "Epoch 2/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.5829\n",
            "Epoch 3/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.3200\n",
            "Epoch 4/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.2375\n",
            "Epoch 5/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.2100\n",
            "Epoch 6/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.2030\n",
            "Epoch 7/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.2023\n",
            "Epoch 8/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.1974\n",
            "Epoch 9/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.1967\n",
            "Epoch 10/10\n",
            "\u001b[1m9/9\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.9524 - loss: 0.1950\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7a6cfc575c50>"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Unpack those dimensions:\n",
        "num_sets, max_reps, max_timesteps, feature_dim = X_padded.shape\n",
        "\n",
        "# 2) Build the model:\n",
        "model = build_two_level_lstm_model(\n",
        "    max_reps=max_reps,\n",
        "    max_timesteps=max_timesteps,\n",
        "    feature_dim=feature_dim,\n",
        "    rep_embedding_dim=32,  # or any dimension you want for the rep embedding\n",
        "    hidden_units=32        # or another size for your outer LSTM\n",
        ")\n",
        "\n",
        "# 3) Prepare labels for training:\n",
        "#    By default, Y.shape is (num_sets, max_reps).\n",
        "#    The model's final output is (batch_size, max_reps, 1) because of the TimeDistributed Dense layer,\n",
        "#    so we need to expand the last dimension of Y:\n",
        "Y_expanded = np.expand_dims(Y, axis=-1)  # (num_sets, max_reps, 1)\n",
        "\n",
        "# If you have -1 for \"padded reps\" in Y, you either set them to 0 or use a masking approach.\n",
        "# For a quick approach, you can do something like:\n",
        "Y_expanded[Y_expanded == -1] = 0\n",
        "\n",
        "# 4) Train the model:\n",
        "model.fit(\n",
        "    X_padded,        # shape: (num_sets, max_reps, max_timesteps, 7)\n",
        "    Y_expanded,      # shape: (num_sets, max_reps, 1)\n",
        "    epochs=10,\n",
        "    batch_size=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ43mgJFTfHI",
        "outputId": "cfa344f0-b5a0-4cb7-ec24-a66a1d8e9671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
            "preds.shape: (17, 21)\n"
          ]
        }
      ],
      "source": [
        "# Analyse predictions\n",
        "\n",
        "# 1) Generate predictions\n",
        "preds = model.predict(X_padded)\n",
        "# preds shape: (num_sets, max_reps, 1) because the final layer is TimeDistributed(Dense(1, ...))\n",
        "\n",
        "# 2) Squeeze the last dimension so we get (num_sets, max_reps)\n",
        "preds = preds.squeeze(-1)\n",
        "print(\"preds.shape:\", preds.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "13Y7gGyvTlUc"
      },
      "outputs": [],
      "source": [
        "predicted_fail_reps = np.argmax(preds, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEyLA45mT5V7",
        "outputId": "b5d46394-5b59-44aa-bbca-7160892fa14f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== SET 0 =====\n",
            "Predicted fail rep index: 0, Probability: 0.107\n",
            "Predicted probabilities for each rep: [0.10700000077486038, 0.0560000017285347, 0.04699999839067459, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459]\n",
            "Ground truth fail rep(s): [18] (label=1)\n",
            "\n",
            "===== SET 1 =====\n",
            "Predicted fail rep index: 0, Probability: 0.107\n",
            "Predicted probabilities for each rep: [0.10700000077486038, 0.0560000017285347, 0.04699999839067459, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459]\n",
            "Ground truth fail rep(s): [10] (label=1)\n",
            "\n",
            "===== SET 2 =====\n",
            "Predicted fail rep index: 0, Probability: 0.107\n",
            "Predicted probabilities for each rep: [0.10700000077486038, 0.0560000017285347, 0.04699999839067459, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459]\n",
            "Ground truth fail rep(s): [19] (label=1)\n",
            "\n",
            "===== SET 3 =====\n",
            "Predicted fail rep index: 0, Probability: 0.107\n",
            "Predicted probabilities for each rep: [0.10700000077486038, 0.0560000017285347, 0.04699999839067459, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459, 0.04699999839067459]\n",
            "Ground truth fail rep(s): [10] (label=1)\n",
            "\n",
            "===== SET 4 =====\n",
            "Predicted fail rep index: 0, Probability: 0.107\n",
            "Predicted probabilities for each rep: [0.10700000077486038, 0.0560000017285347, 0.04699999839067459, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.04600000008940697, 0.06499999761581421, 0.04699999839067459]\n",
            "Ground truth fail rep(s): [8] (label=1)\n"
          ]
        }
      ],
      "source": [
        "num_sets = X_padded.shape[0]\n",
        "\n",
        "for i in range(min(5, num_sets)):\n",
        "    # Show the predicted probabilities for set i\n",
        "    set_preds = preds[i]            # shape: (max_reps,)\n",
        "    set_labels = Y[i]               # shape: (max_reps,) or (max_reps, 1)\n",
        "    actual_reps = rep_counts[i]     # how many reps this set actually has (before padding)\n",
        "\n",
        "    # If necessary, slice to the actual reps\n",
        "    set_preds = set_preds[:actual_reps]\n",
        "    set_labels = set_labels[:actual_reps]\n",
        "\n",
        "    # Find the model's predicted fail rep (the rep with highest probability)\n",
        "    fail_index = np.argmax(set_preds)\n",
        "\n",
        "    print(f\"\\n===== SET {i} =====\")\n",
        "    print(f\"Predicted fail rep index: {fail_index}, Probability: {set_preds[fail_index]:.3f}\")\n",
        "    print(\"Predicted probabilities for each rep:\", np.round(set_preds, 3).tolist())\n",
        "\n",
        "    # Compare with ground truth (where is label=1?)\n",
        "    ground_truth_fail = np.where(set_labels == 1)[0]\n",
        "    if len(ground_truth_fail) > 0:\n",
        "        print(f\"Ground truth fail rep(s): {ground_truth_fail} (label=1)\")\n",
        "    else:\n",
        "        print(\"No ground truth fail rep found (possible mismatch or data issue).\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
