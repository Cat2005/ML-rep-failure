{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe numpy\n",
        "!pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2uE30coO-59",
        "outputId": "dfd24a43-e2ea-453a-94e0-1512f55f8cdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.14.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO5y0UpSO-eV"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy\n",
        "import os\n",
        "import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DRIVE_PATH = \"/content/drive/mlp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeT3XkjuQE9D",
        "outputId": "18095dc5-c56c-4360-93d6-a334e9955d70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU6oaACIO-eW"
      },
      "outputs": [],
      "source": [
        "# Helper function to calculate the angle between three points.\n",
        "def calculate_angle(a, b, c):\n",
        "    \"\"\"\n",
        "    Calculates the angle (in degrees) between the line segments ab and bc.\n",
        "\n",
        "    Args:\n",
        "        a, b, c (list or array): [x, y] coordinates.\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated angle in degrees.\n",
        "    \"\"\"\n",
        "    a = np.array(a)\n",
        "    b = np.array(b)\n",
        "    c = np.array(c)\n",
        "\n",
        "    ba = a - b\n",
        "    bc = c - b\n",
        "\n",
        "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    return np.degrees(angle)\n",
        "\n",
        "# Determine which arm to use based on the z-coordinates of the shoulders.\n",
        "def determine_arm(frame, pose_model):\n",
        "    \"\"\"\n",
        "    Determines which arm to use for analysis (left or right) by comparing\n",
        "    the z-coordinates of the left and right shoulders.\n",
        "\n",
        "    Args:\n",
        "        frame (numpy.ndarray): A single video frame in BGR format.\n",
        "        pose_model: An instance of the Mediapipe Pose model.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the right arm should be used, False for the left.\n",
        "    \"\"\"\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose_model.process(image_rgb)\n",
        "    use_right_arm = True  # Default to right\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        landmarks = results.pose_landmarks.landmark\n",
        "        if landmarks[mp.solutions.pose.PoseLandmark.LEFT_SHOULDER].z < \\\n",
        "           landmarks[mp.solutions.pose.PoseLandmark.RIGHT_SHOULDER].z:\n",
        "            use_right_arm = False\n",
        "\n",
        "    return use_right_arm\n",
        "\n",
        "# Process video frames and extract the elbow angles along with timestamps\n",
        "# and annotated frames.\n",
        "def process_video_frames(video_path):\n",
        "    \"\"\"\n",
        "    Processes video frames to extract the elbow angle using Mediapipe.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video.\n",
        "\n",
        "    Returns:\n",
        "        timestamps (np.array): Array of time stamps.\n",
        "        angles (np.array): Array of computed elbow angles.\n",
        "        annotated_frames (list): List of tuples (timestamp, frame) after drawing.\n",
        "        fps (int): Frames per second in the input video.\n",
        "        frame_width (int): Width of the video frames.\n",
        "        frame_height (int): Height of the video frames.\n",
        "    \"\"\"\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose_model = mp_pose.Pose()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Read the first frame to decide which arm to track.\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        pose_model.close()\n",
        "        raise ValueError(\"Unable to read video frame\")\n",
        "\n",
        "    use_right_arm = determine_arm(first_frame, pose_model)\n",
        "    shoulder_landmark = (mp_pose.PoseLandmark.RIGHT_SHOULDER if use_right_arm\n",
        "                         else mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
        "    elbow_landmark = (mp_pose.PoseLandmark.RIGHT_ELBOW if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_ELBOW)\n",
        "    wrist_landmark = (mp_pose.PoseLandmark.RIGHT_WRIST if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_WRIST)\n",
        "\n",
        "    # Reset to the beginning of the video.\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "    angles = []\n",
        "    timestamps = []\n",
        "    annotated_frames = []\n",
        "    frame_idx = 0\n",
        "    print(\"Processing video frames...\")\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        timestamp = frame_idx / fps\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose_model.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            shoulder = [landmarks[shoulder_landmark.value].x * frame_width,\n",
        "                        landmarks[shoulder_landmark.value].y * frame_height]\n",
        "            elbow = [landmarks[elbow_landmark.value].x * frame_width,\n",
        "                     landmarks[elbow_landmark.value].y * frame_height]\n",
        "            wrist = [landmarks[wrist_landmark.value].x * frame_width,\n",
        "                     landmarks[wrist_landmark.value].y * frame_height]\n",
        "\n",
        "            angle = calculate_angle(shoulder, elbow, wrist)\n",
        "            angles.append(angle)\n",
        "            timestamps.append(timestamp)\n",
        "\n",
        "            # Draw keypoints.\n",
        "            cv2.circle(frame, tuple(map(int, shoulder)), 8, (0, 255, 0), -1)\n",
        "            cv2.circle(frame, tuple(map(int, elbow)), 8, (255, 0, 0), -1)\n",
        "            cv2.circle(frame, tuple(map(int, wrist)), 8, (0, 0, 255), -1)\n",
        "\n",
        "            # Draw connecting lines.\n",
        "            cv2.line(frame, tuple(map(int, shoulder)),\n",
        "                     tuple(map(int, elbow)), (255, 255, 255), 2)\n",
        "            cv2.line(frame, tuple(map(int, elbow)),\n",
        "                     tuple(map(int, wrist)), (255, 255, 255), 2)\n",
        "\n",
        "            # Display the elbow angle.\n",
        "            cv2.putText(frame, f\"Angle: {int(angle)} deg\",\n",
        "                        (int(elbow[0]) + 20, int(elbow[1]) - 10),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        annotated_frames.append((timestamp, frame))\n",
        "        print(f\"\\rProgress: {(frame_idx/total_frames)*100:.1f}%\", end=\"\")\n",
        "\n",
        "    print(\"\\n\")\n",
        "    cap.release()\n",
        "    pose_model.close()\n",
        "\n",
        "    return np.array(timestamps), np.array(angles), annotated_frames, fps, frame_width, frame_height\n",
        "\n",
        "\n",
        "from scipy.signal import find_peaks\n",
        "\n",
        "def detect_reps(angles, timestamps, sigma=10, min_distance=50, min_valley_diff=60):\n",
        "    \"\"\"\n",
        "    Detects bicep curl repetitions by identifying peaks (high points) and valleys\n",
        "    (low points) dynamically.\n",
        "\n",
        "    Args:\n",
        "        angles (list or np.array): The elbow angles over time.\n",
        "        timestamps (list or np.array): The corresponding timestamps.\n",
        "        sigma (float): Smoothing factor for the angles.\n",
        "        min_distance (int): Minimum distance between peaks/valleys (in samples).\n",
        "        min_valley_diff (int): Minimum angle difference required between peak and valley.\n",
        "\n",
        "    Returns:\n",
        "        rep_segments (list): List of tuples (start_time, end_time) for each rep.\n",
        "        angles_smoothed (np.array): The smoothed angle data.\n",
        "    \"\"\"\n",
        "    # Smooth the angles to reduce noise\n",
        "    angles_smoothed = scipy.ndimage.gaussian_filter1d(angles, sigma=sigma)\n",
        "\n",
        "    # Find peaks (local maxima) and valleys (local minima)\n",
        "    peaks, _ = find_peaks(angles_smoothed, distance=min_distance, height=90)  # Only peaks above 90 degrees\n",
        "    valleys, _ = find_peaks(-angles_smoothed, distance=min_distance, height=(-90))  # Inverted signal for valleys, must be below 90\n",
        "\n",
        "    # Validate peaks and valleys\n",
        "    valid_peaks = []\n",
        "    valid_valleys = []\n",
        "    for i in range(len(peaks)-1):\n",
        "        # Find valleys between consecutive peaks\n",
        "        valley_candidates = [v for v in valleys if peaks[i] < v < peaks[i+1]]\n",
        "\n",
        "        if valley_candidates:\n",
        "            # If multiple valleys exist, keep only the one with highest angle (shallowest valley)\n",
        "            valley = max(valley_candidates, key=lambda v: angles_smoothed[v])\n",
        "\n",
        "            # Check if valley is deep enough compared to both peaks\n",
        "            peak1_to_valley = angles_smoothed[peaks[i]] - angles_smoothed[valley]\n",
        "            valley_to_peak2 = angles_smoothed[peaks[i+1]] - angles_smoothed[valley]\n",
        "\n",
        "            if peak1_to_valley >= min_valley_diff and valley_to_peak2 >= min_valley_diff:\n",
        "                valid_peaks.append(peaks[i])\n",
        "                valid_valleys.append(valley)\n",
        "                if i == len(peaks)-2:  # Add the last peak if its valley was valid\n",
        "                    valid_peaks.append(peaks[i+1])\n",
        "\n",
        "    # Create rep segments from valid peaks\n",
        "    rep_segments = []\n",
        "    if valid_peaks:\n",
        "        # First segment starts from beginning to first valid peak\n",
        "        rep_segments.append((timestamps[0], timestamps[valid_peaks[0]]))\n",
        "\n",
        "        # Middle segments\n",
        "        for i in range(len(valid_peaks)-1):\n",
        "            rep_segments.append((timestamps[valid_peaks[i]], timestamps[valid_peaks[i+1]]))\n",
        "\n",
        "        # Last segment from last valid peak to end\n",
        "        rep_segments.append((timestamps[valid_peaks[-1]], timestamps[-1]))\n",
        "\n",
        "    return rep_segments\n",
        "\n",
        "# Plot the elbow angle versus time and mark the detected repetitions.\n",
        "def plot_elbow_angle(timestamps, angles_smoothed, rep_segments, file_name):\n",
        "    \"\"\"\n",
        "    Plots the smoothed elbow angle over time and highlights the periods\n",
        "    considered as repetitions.\n",
        "\n",
        "    Args:\n",
        "        timestamps (np.array): Array of timestamps.\n",
        "        angles_smoothed (np.array): Smoothed elbow angle values.\n",
        "        rep_segments (list): List of rep segments as (start_time, end_time).\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    plt.plot(timestamps, angles_smoothed, 'b-', label='Elbow Angle')\n",
        "\n",
        "    colors=['green', 'red']\n",
        "\n",
        "    # Highlight the periods considered as repetitions\n",
        "    for i, (start, end) in enumerate(rep_segments):\n",
        "        plt.axvspan(start, end, color=colors[i % len(colors)], alpha=0.3, label='Rep Period')\n",
        "\n",
        "    # Add labels and legend\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('Elbow Angle (degrees)')\n",
        "    plt.title(f'Bicep Curl Analysis - {file_name}')\n",
        "    plt.legend(['Elbow Angle', 'Rep Period'], loc='upper right')\n",
        "    plt.grid(True)\n",
        "    plt.gca().invert_yaxis()  # Reverse the y-axis\n",
        "    plt.show()\n",
        "\n",
        "# Overlay repetition markers on the annotated video frames.\n",
        "def overlay_rep_markers(annotated_frames, rep_segments, fps):\n",
        "    \"\"\"\n",
        "    Overlays rep start and end markers on each annotated frame.\n",
        "\n",
        "    Args:\n",
        "        annotated_frames (list): List of tuples (timestamp, frame).\n",
        "        rep_segments (list): List of rep segments as (start_time, end_time).\n",
        "        fps (int): Frames per second.\n",
        "\n",
        "    Returns:\n",
        "        List of frames with overlay annotations.\n",
        "    \"\"\"\n",
        "    updated_frames = []\n",
        "    for timestamp, frame in annotated_frames:\n",
        "        # Find current rep\n",
        "        current_rep = 0\n",
        "        for i, (start, end) in enumerate(rep_segments):\n",
        "            if start <= timestamp <= end:\n",
        "                current_rep = i + 1\n",
        "            if abs(timestamp - start) < 1 / fps:\n",
        "                cv2.circle(frame, (50, 50), 15, (0, 255, 0), -1)\n",
        "                cv2.putText(frame, f\"Start Rep {i+1}\", (70, 55),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "            if abs(timestamp - end) < 1 / fps:\n",
        "                cv2.circle(frame, (50, 100), 15, (0, 0, 255), -1)\n",
        "                cv2.putText(frame, f\"End Rep {i+1}\", (70, 105),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "\n",
        "        # Draw stats overlay\n",
        "        y_offset = 30\n",
        "        cv2.putText(frame, f\"Time: {timestamp:.1f}s\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        y_offset += 25\n",
        "        cv2.putText(frame, f\"Current Rep: {current_rep}\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        y_offset += 25\n",
        "        cv2.putText(frame, f\"Total Reps: {len(rep_segments)}\", (10, y_offset),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "        if current_rep > 0 and current_rep <= len(rep_segments):\n",
        "            start, end = rep_segments[current_rep-1]\n",
        "            y_offset += 25\n",
        "            cv2.putText(frame, f\"Rep Duration: {end-start:.1f}s\", (10, y_offset),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
        "\n",
        "        updated_frames.append(frame)\n",
        "    return updated_frames\n",
        "\n",
        "# Write the annotated frames to a video file.\n",
        "def write_video(frames, output_path, fps, frame_width, frame_height):\n",
        "    \"\"\"\n",
        "    Writes a list of frames to a video file.\n",
        "\n",
        "    Args:\n",
        "        frames (list): List of frames (images).\n",
        "        output_path (str): Path to save the output video.\n",
        "        fps (int): Frames per second.\n",
        "        frame_width (int): Width of video frames.\n",
        "        frame_height (int): Height of video frames.\n",
        "    \"\"\"\n",
        "    out = cv2.VideoWriter(\n",
        "        output_path,\n",
        "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "        fps,\n",
        "        (frame_width, frame_height)\n",
        "    )\n",
        "    for frame in frames:\n",
        "        out.write(frame)\n",
        "    out.release()\n",
        "    print(f\"✅ Processed video saved as `{output_path}`\")\n",
        "\n",
        "# Main routine that ties everything together.\n",
        "def run_bicep_curl_analysis(video_path, output_path=\"bicep_curl_debug.mp4\"):\n",
        "    \"\"\"\n",
        "    Orchestrates the bicep curl analysis pipeline:\n",
        "      1. Process video frames to extract angles.\n",
        "      2. Detect reps via signal processing.\n",
        "      3. Plot the elbow angle versus time.\n",
        "      4. Overlay rep markers on the video.\n",
        "      5. Write the annotated video to disk.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video.\n",
        "        output_path (str): Path to save the annotated output video.\n",
        "\n",
        "    Returns:\n",
        "        list: List of rep segments (each as (start_time, end_time)).\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "        process_video_frames(video_path)\n",
        "    rep_segments = detect_reps(angles, timestamps)\n",
        "\n",
        "    print(\"\\nDetected reps:\")\n",
        "    for i, (start, end) in enumerate(rep_segments):\n",
        "        print(f\"Rep {i+1}: Start = {start:.2f}s, End = {end:.2f}s, \"\n",
        "              f\"Duration = {end - start:.2f}s\")\n",
        "\n",
        "    plot_elbow_angle(timestamps, angles, rep_segments, video_path)\n",
        "    frames_with_overlay = overlay_rep_markers(annotated_frames, rep_segments, fps)\n",
        "    write_video(frames_with_overlay, output_path, fps, frame_width, frame_height)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"\\nTotal processing time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    return rep_segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgyMV7GBO-eY"
      },
      "outputs": [],
      "source": [
        "# # timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "# #         process_video_frames(\"./data/12r (1).mp4\")\n",
        "\n",
        "# files = os.listdir(\"./data\")\n",
        "# print(files)\n",
        "\n",
        "# # for each file in ./data folder, run the analyze_bicep_curls function\n",
        "# analysis_results = []\n",
        "# for file in files:\n",
        "#     if file.endswith(\".mp4\"):\n",
        "#         print(f\"Analyzing {file}\")\n",
        "#         timestamps, angles, annotated_frames, fps, frame_width, frame_height = \\\n",
        "#             process_video_frames(\"./data/\" + file)\n",
        "#         analysis_results.append((file, timestamps, angles, fps, frame_width, frame_height))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPT3-70OO-ea"
      },
      "outputs": [],
      "source": [
        "# for file, timestamps, angles, fps, frame_width, frame_height in analysis_results:\n",
        "#     rep_segments = detect_reps(angles, timestamps)\n",
        "#     plot_elbow_angle(timestamps, angles, rep_segments, \"./data/\" + file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrYCTV-UO-ea"
      },
      "outputs": [],
      "source": [
        "# files = os.listdir(\"./data\")\n",
        "# print(files)\n",
        "\n",
        "# # for each file in ./data folder, run the analyze_bicep_curls function\n",
        "# for file in files:\n",
        "#     if file.endswith(\".mp4\") and \"t\" in file:\n",
        "#         print(f\"Analyzing {file}\")\n",
        "#         run_bicep_curl_analysis(f\"./data/{file}\", f\"./out/{file}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_rep_joint_data(video_path, normalize=True):\n",
        "    \"\"\"\n",
        "    Extracts joint coordinate data for each repetition from a video.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): Path to the input video\n",
        "        normalize (bool): Whether to normalize coordinates relative to shoulder position\n",
        "\n",
        "    Returns:\n",
        "        list: List of repetitions, where each rep is a numpy array of shape\n",
        "             (timesteps, n_features). Features are [elbow_x, elbow_y, wrist_x, wrist_y]\n",
        "             or their normalized versions relative to shoulder position.\n",
        "    \"\"\"\n",
        "    # Process video to get landmarks\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose_model = mp_pose.Pose()\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Determine which arm to use\n",
        "    ret, first_frame = cap.read()\n",
        "    if not ret:\n",
        "        cap.release()\n",
        "        pose_model.close()\n",
        "        raise ValueError(\"Unable to read video frame\")\n",
        "\n",
        "    use_right_arm = determine_arm(first_frame, pose_model)\n",
        "    shoulder_landmark = (mp_pose.PoseLandmark.RIGHT_SHOULDER if use_right_arm\n",
        "                        else mp_pose.PoseLandmark.LEFT_SHOULDER)\n",
        "    elbow_landmark = (mp_pose.PoseLandmark.RIGHT_ELBOW if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_ELBOW)\n",
        "    wrist_landmark = (mp_pose.PoseLandmark.RIGHT_WRIST if use_right_arm\n",
        "                      else mp_pose.PoseLandmark.LEFT_WRIST)\n",
        "\n",
        "    # Reset video\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    # Extract coordinates and angles\n",
        "    coords = []\n",
        "    angles = []\n",
        "    timestamps = []\n",
        "    frame_idx = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame_idx += 1\n",
        "        timestamp = frame_idx / fps\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = pose_model.process(image_rgb)\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            landmarks = results.pose_landmarks.landmark\n",
        "\n",
        "            shoulder = np.array([\n",
        "                landmarks[shoulder_landmark.value].x * frame_width,\n",
        "                landmarks[shoulder_landmark.value].y * frame_height\n",
        "            ])\n",
        "            elbow = np.array([\n",
        "                landmarks[elbow_landmark.value].x * frame_width,\n",
        "                landmarks[elbow_landmark.value].y * frame_height\n",
        "            ])\n",
        "            wrist = np.array([\n",
        "                landmarks[wrist_landmark.value].x * frame_width,\n",
        "                landmarks[wrist_landmark.value].y * frame_height\n",
        "            ])\n",
        "\n",
        "            if normalize:\n",
        "                # Normalize coordinates relative to shoulder position\n",
        "                elbow = elbow - shoulder\n",
        "                wrist = wrist - shoulder\n",
        "\n",
        "            # Store coordinates as [elbow_x, elbow_y, wrist_x, wrist_y]\n",
        "            coords.append(np.concatenate([elbow, wrist]))\n",
        "\n",
        "            angle = calculate_angle(shoulder, elbow + shoulder, wrist + shoulder if normalize else wrist)\n",
        "            angles.append(angle)\n",
        "            timestamps.append(timestamp)\n",
        "\n",
        "    cap.release()\n",
        "    pose_model.close()\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    coords = np.array(coords)\n",
        "    angles = np.array(angles)\n",
        "    timestamps = np.array(timestamps)\n",
        "\n",
        "    # Detect repetitions\n",
        "    rep_segments = detect_reps(angles, timestamps)\n",
        "\n",
        "    # Split coordinates into repetitions\n",
        "    reps_data = []\n",
        "    for start_time, end_time in rep_segments:\n",
        "        start_idx = np.argmin(np.abs(timestamps - start_time))\n",
        "        end_idx = np.argmin(np.abs(timestamps - end_time))\n",
        "        rep_coords = coords[start_idx:end_idx+1]\n",
        "        reps_data.append(rep_coords)\n",
        "\n",
        "    return reps_data\n",
        "\n"
      ],
      "metadata": {
        "id": "tea_Urmlt5H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpNxidLt_5m",
        "outputId": "9adee38b-578c-4778-d150-2a706a1ec55d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FiUH5c0O-ea"
      },
      "outputs": [],
      "source": [
        "\n",
        "def process_all_videos(data_dir):\n",
        "    \"\"\"\n",
        "    Process all videos in a directory and return their rep data.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Path to directory containing videos\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping filenames to lists of rep data arrays\n",
        "    \"\"\"\n",
        "    all_sets_data = {}\n",
        "\n",
        "    for file in os.listdir(data_dir)[1:]:\n",
        "        if file.endswith('.mp4'):\n",
        "            print(f\"Processing {file}...\")\n",
        "            video_path = os.path.join(data_dir, file)\n",
        "            sets_data = extract_rep_joint_data(video_path)\n",
        "            all_sets_data[file] = sets_data\n",
        "            print(f\"Found {len(sets_data)} reps in {file}\")\n",
        "    return all_sets_data\n",
        "\n",
        "    # Get the first video file\n",
        "    # files = [f for f in os.listdir(data_dir) if f.endswith('.mp4')]\n",
        "    # if files:\n",
        "    #     first_file = files[0]\n",
        "    #     print(f\"Processing {first_file}...\")\n",
        "    #     video_path = os.path.join(data_dir, first_file)\n",
        "    #     reps_data = extract_rep_joint_data(video_path)\n",
        "\n",
        "    #     # Print info about the extracted data\n",
        "    #     print(f\"\\nFound {len(reps_data)} reps in {first_file}\")\n",
        "    #     for i, rep in enumerate(reps_data):\n",
        "    #         print(f\"Rep {i+1} shape: {rep.shape}\")\n",
        "    # else:\n",
        "    #     print(\"No video files found in ./data directory\")\n",
        "\n",
        "    # return reps_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_video_reps(video_name, reps_data, save_dir='processed_data_npzs'):\n",
        "    \"\"\"\n",
        "    Save rep data for a single video to a .npz file.\n",
        "\n",
        "    Args:\n",
        "        video_name (str): Name of the video file\n",
        "        reps_data (list): List of numpy arrays containing rep data\n",
        "        save_dir (str): Directory to save the .npz files\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    save_path = os.path.join(save_dir, video_name.replace('.mp4', '.npz'))\n",
        "    save_dict = {f'rep_{i}': rep for i, rep in enumerate(reps_data)}\n",
        "    np.savez_compressed(save_path, **save_dict)\n",
        "\n",
        "def process_videos_directory(data_dir, save_dir='processed_npzs'):\n",
        "    \"\"\"\n",
        "    Process all videos in directory, skipping those that already have .npz files.\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory containing videos\n",
        "        save_dir (str): Directory to save processed data\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of processed videos and their rep data\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    all_sets_data = {}\n",
        "    print(len(os.listdir(data_dir)))\n",
        "\n",
        "    for file in os.listdir(data_dir):\n",
        "        if file.endswith('.mp4') or file.endswith('.mov'):\n",
        "            if file.endswith('.mp4'):\n",
        "              npz_path = os.path.join(save_dir, file.replace('.mp4', '.npz'))\n",
        "            else:\n",
        "              npz_path = os.path.join(save_dir, file.replace('.mov', '.mov.npz'))\n",
        "\n",
        "            if os.path.exists(npz_path):\n",
        "                print(f\"Skipping {file} - already processed\")\n",
        "                continue\n",
        "\n",
        "            print(f\"Processing {file}...\")\n",
        "            video_path = os.path.join(data_dir, file)\n",
        "            set_data = extract_rep_joint_data(video_path)\n",
        "            all_sets_data[file] = set_data\n",
        "\n",
        "            # Save the processed data\n",
        "            save_video_reps(file, set_data, save_dir)\n",
        "            print(f\"Found and saved {len(set_data)} reps for {file}\")\n",
        "\n",
        "    return all_sets_data\n",
        "\n",
        "def save_existing_reps_data(all_reps_data, save_dir='processed_npzs'):\n",
        "    \"\"\"\n",
        "    Save any rep data currently in memory that hasn't been saved yet.\n",
        "\n",
        "    Args:\n",
        "        all_reps_data (dict): Dictionary mapping video names to rep data\n",
        "        save_dir (str): Directory to save processed data\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for video_name, set_data in all_reps_data.items():\n",
        "        npz_path = os.path.join(save_dir, video_name.replace('.mp4', '.npz'))\n",
        "        if not os.path.exists(npz_path):\n",
        "            print(f\"Saving data for {video_name}...\")\n",
        "            save_video_reps(video_name, set_data, save_dir)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_all_processed_data(processed_dir='processed_npzs'):\n",
        "    \"\"\"\n",
        "    Load all processed rep data from .npz files in directory.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary mapping video names to a list of rep arrays.\n",
        "              Each rep array should ideally have shape (timesteps, 4).\n",
        "    \"\"\"\n",
        "    all_sets_data = {}\n",
        "\n",
        "    for file in os.listdir(processed_dir):\n",
        "        if file.endswith('.npz'):\n",
        "            video_name = file.replace('.npz', '.mp4')\n",
        "            npz_path = os.path.join(processed_dir, file)\n",
        "\n",
        "            loaded = np.load(npz_path)\n",
        "\n",
        "            # Let's store arrays in a list\n",
        "            reps = []\n",
        "            for key in sorted(loaded.files):\n",
        "                arr = loaded[key]\n",
        "\n",
        "                # Debug info: check array type and shape\n",
        "                print(f\"{file} -> key={key}, arr.shape={arr.shape}, arr.dtype={arr.dtype}\")\n",
        "\n",
        "                reps.append(arr)\n",
        "\n",
        "            all_sets_data[video_name] = reps\n",
        "            print(f\"Loaded {len(reps)} reps from {video_name}\")\n",
        "\n",
        "    return all_sets_data\n"
      ],
      "metadata": {
        "id": "Po5yCfnz9v_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sets_data = process_videos_directory(\"./data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbshEQ0Vgn5v",
        "outputId": "42fdfdfa-46fb-4253-854c-47ea72537ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n",
            "Skipping 4_r.mp4 - already processed\n",
            "Skipping t5.mp4 - already processed\n",
            "Skipping t1.mp4 - already processed\n",
            "Skipping t2.mp4 - already processed\n",
            "Skipping 5_l.mp4 - already processed\n",
            "Skipping 14r (1).mp4 - already processed\n",
            "Skipping 11r (1).mp4 - already processed\n",
            "Skipping 12r (1).mp4 - already processed\n",
            "Skipping 5_r.mp4 - already processed\n",
            "Skipping 10r (1).mp4 - already processed\n",
            "Skipping 8_r (1).mp4 - already processed\n",
            "Skipping t4.mp4 - already processed\n",
            "Skipping 6_l.mp4 - already processed\n",
            "Skipping t6.mp4 - already processed\n",
            "Skipping 9r (1).mp4 - already processed\n",
            "Skipping 7_r (1).mp4 - already processed\n",
            "Skipping t3.mp4 - already processed\n",
            "Skipping 13r (1).mp4 - already processed\n",
            "processed_npzs/IMG_5711.mov.npz\n",
            "Skipping IMG_5711.mov - already processed\n",
            "processed_npzs/IMG_5712.mov.npz\n",
            "Skipping IMG_5712.mov - already processed\n",
            "processed_npzs/IMG_5714.mov.npz\n",
            "Skipping IMG_5714.mov - already processed\n",
            "processed_npzs/IMG_5713.mov.npz\n",
            "Skipping IMG_5713.mov - already processed\n",
            "processed_npzs/IMG_5717.mov.npz\n",
            "Skipping IMG_5717.mov - already processed\n",
            "processed_npzs/IMG_5715.mov.npz\n",
            "Skipping IMG_5715.mov - already processed\n",
            "processed_npzs/IMG_5704.mov.npz\n",
            "Skipping IMG_5704.mov - already processed\n",
            "processed_npzs/IMG_5706.mov.npz\n",
            "Skipping IMG_5706.mov - already processed\n",
            "processed_npzs/IMG_5705.mov.npz\n",
            "Skipping IMG_5705.mov - already processed\n",
            "processed_npzs/IMG_5703.mov.npz\n",
            "Skipping IMG_5703.mov - already processed\n",
            "processed_npzs/IMG_5707.mov.npz\n",
            "Skipping IMG_5707.mov - already processed\n",
            "processed_npzs/IMG_5708.mov.npz\n",
            "Skipping IMG_5708.mov - already processed\n",
            "processed_npzs/IMG_5718.mov.npz\n",
            "Skipping IMG_5718.mov - already processed\n",
            "processed_npzs/IMG_5722.mov.npz\n",
            "Skipping IMG_5722.mov - already processed\n",
            "processed_npzs/IMG_5721.mov.npz\n",
            "Skipping IMG_5721.mov - already processed\n",
            "processed_npzs/IMG_5720.mov.npz\n",
            "Skipping IMG_5720.mov - already processed\n",
            "processed_npzs/IMG_5723.mov.npz\n",
            "Skipping IMG_5723.mov - already processed\n",
            "processed_npzs/IMG_5724.mov.npz\n",
            "Skipping IMG_5724.mov - already processed\n",
            "processed_npzs/IMG_5725.mov.npz\n",
            "Skipping IMG_5725.mov - already processed\n",
            "processed_npzs/IMG_5726.mov.npz\n",
            "Skipping IMG_5726.mov - already processed\n",
            "processed_npzs/IMG_5727.mov.npz\n",
            "Skipping IMG_5727.mov - already processed\n",
            "processed_npzs/IMG_5728.mov.npz\n",
            "Skipping IMG_5728.mov - already processed\n",
            "processed_npzs/IMG_5729.mov.npz\n",
            "Skipping IMG_5729.mov - already processed\n",
            "processed_npzs/IMG_5732.mov.npz\n",
            "Skipping IMG_5732.mov - already processed\n",
            "processed_npzs/IMG_5731.mov.npz\n",
            "Skipping IMG_5731.mov - already processed\n",
            "processed_npzs/IMG_5730.mov.npz\n",
            "Skipping IMG_5730.mov - already processed\n",
            "processed_npzs/IMG_5733.mov.npz\n",
            "Skipping IMG_5733.mov - already processed\n",
            "processed_npzs/IMG_5734.mov.npz\n",
            "Skipping IMG_5734.mov - already processed\n",
            "processed_npzs/IMG_5737.mov.npz\n",
            "Skipping IMG_5737.mov - already processed\n",
            "processed_npzs/IMG_5735.mov.npz\n",
            "Skipping IMG_5735.mov - already processed\n",
            "processed_npzs/IMG_5736.mov.npz\n",
            "Skipping IMG_5736.mov - already processed\n",
            "processed_npzs/IMG_5738.mov.npz\n",
            "Skipping IMG_5738.mov - already processed\n",
            "processed_npzs/IMG_5741.mov.npz\n",
            "Skipping IMG_5741.mov - already processed\n",
            "processed_npzs/IMG_5739.mov.npz\n",
            "Skipping IMG_5739.mov - already processed\n",
            "processed_npzs/IMG_5740.mov.npz\n",
            "Skipping IMG_5740.mov - already processed\n",
            "processed_npzs/IMG_7773.mov.npz\n",
            "Processing IMG_7773.mov...\n",
            "Found and saved 0 reps for IMG_7773.mov\n",
            "processed_npzs/IMG_7770.mov.npz\n",
            "Skipping IMG_7770.mov - already processed\n",
            "processed_npzs/IMG_7772.mov.npz\n",
            "Processing IMG_7772.mov...\n",
            "Found and saved 3 reps for IMG_7772.mov\n",
            "processed_npzs/IMG_7769.mov.npz\n",
            "Processing IMG_7769.mov...\n",
            "Found and saved 0 reps for IMG_7769.mov\n",
            "processed_npzs/IMG_7768.mov.npz\n",
            "Processing IMG_7768.mov...\n",
            "Found and saved 5 reps for IMG_7768.mov\n",
            "processed_npzs/IMG_7767.mov.npz\n",
            "Processing IMG_7767.mov...\n",
            "Found and saved 13 reps for IMG_7767.mov\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}